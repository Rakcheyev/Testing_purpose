"""Local pilot workflow for processing curated PBIP cases without HTTP APIs."""

from __future__ import annotations

import argparse
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, List

from mcp_server.orchestration import SessionManager, AuditTrail

BASE_DIR = Path(__file__).resolve().parent
ARTIFACTS_ROOT = Path("pbip_artifacts") / "pilot_case"
STAGE_ROOT = BASE_DIR / "pilot_case"

PIPELINE_STEPS = (
    ("ingest", "Collected PBIP source and metadata"),
    ("lint", "Executed lint checklist"),
    ("standardize", "Applied naming and folder standards"),
    ("report", "Generated summary report"),
)


def isoformat(epoch: float) -> str:
    return datetime.fromtimestamp(epoch, timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def load_metadata(case_dir: Path) -> Dict[str, Any]:
    metadata_path = case_dir / "metadata.json"
    if metadata_path.exists():
        return json.loads(metadata_path.read_text())
    return {"case_id": case_dir.name, "missing_metadata": True}


def discover_sources(input_dir: Path) -> List[Path]:
    files = sorted(input_dir.glob("*.pbip"))
    files.extend(sorted(input_dir.glob("*.json")))
    return files


def write_artifact(path: Path, payload: Dict[str, Any]):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2, ensure_ascii=False))


def run_case(case_id: str, dry_run: bool = False) -> Dict[str, Any]:
    case_dir = STAGE_ROOT / case_id
    if not case_dir.exists():
        raise FileNotFoundError(f"Case '{case_id}' is not defined under {STAGE_ROOT}")

    metadata = load_metadata(case_dir)
    input_dir = case_dir / "input"
    sources = discover_sources(input_dir)

    audit = AuditTrail()
    manager = SessionManager(audit=audit)
    session_id = manager.start_session(
        user="pilot_cli",
        metadata={"case_id": case_id, "source_count": len(sources)}
    )

    summary: Dict[str, Any] = {
        "case_id": case_id,
        "session_id": session_id,
        "metadata": metadata,
        "sources": [],
        "dry_run": dry_run,
    }

    artifacts_dir = ARTIFACTS_ROOT / case_id
    artifacts_dir.mkdir(parents=True, exist_ok=True)

    if not sources:
        manager.process_session(
            session_id,
            action="no_sources",
            user="pilot_cli",
            payload={"case": case_id},
        )
        summary["note"] = "No PBIP sources were discovered in the input directory."

    for source in sources:
        file_summary = {
            "file": source.name,
            "steps": []
        }
        for action, description in PIPELINE_STEPS:
            entry = manager.process_session(
                session_id,
                action=action,
                user="pilot_cli",
                payload={"file": source.name, "description": description},
            )
            recorded = {
                "action": action,
                "status": entry["status"],
                "description": description,
                "timestamp": isoformat(entry["timestamp"]),
            }
            file_summary["steps"].append(recorded)

        if not dry_run:
            report_payload = {
                "case": case_id,
                "file": source.name,
                "generated_at": isoformat(datetime.now(timezone.utc).timestamp()),
                "notes": "Stub report generated by pilot pipeline",
            }
            report_file = artifacts_dir / f"{source.stem}_report.json"
            write_artifact(report_file, report_payload)
        summary["sources"].append(file_summary)

    manager.close_session(session_id, user="pilot_cli")

    session_history = manager.sessions[session_id]["history"]
    history_payload = [
        {**record, "timestamp": isoformat(record["timestamp"])}
        for record in session_history
    ]
    audit_payload = [
        {
            **record,
            "timestamp": isoformat(record["timestamp"])
        }
        for record in audit.get_session_records(session_id)
    ]

    write_artifact(artifacts_dir / "session_history.json", {"history": history_payload})
    write_artifact(artifacts_dir / "audit.json", {"audit": audit_payload})
    write_artifact(artifacts_dir / "summary.json", summary)

    return summary


def main():
    parser = argparse.ArgumentParser(description="Run local PBIP pilot workflow")
    parser.add_argument("--case", required=True, help="Case identifier under pbip_staging/pilot_case")
    parser.add_argument("--dry-run", action="store_true", help="Skip artifact generation, keep logging only")
    args = parser.parse_args()

    summary = run_case(args.case, dry_run=args.dry_run)
    print(json.dumps({"status": "completed", "case": summary["case_id"], "sources": len(summary["sources"])}))


if __name__ == "__main__":
    main()
